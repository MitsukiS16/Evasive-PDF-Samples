{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evasive PDF Samples\n",
    "\n",
    "Based on https://www.kaggle.com/datasets/fouadtrad2/evasive-pdf-samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Context\n",
    "\n",
    "This dataset is a collection of evasive PDF samples, labeled as malicious (1) or benign (0).  Since the dataset has an evasive nature, it can be used to test the robustness of trained PDF malware classifiers against evasion attacks. The dataset contains 500,000 generated evasive samples, including 450,000 malicious and 50,000 benign PDFs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Objective\n",
    "\n",
    "The primary objective is to create a machine learning application that classifies PDF samples as malicious or benign using various supervised learning algorithms.\n",
    "\n",
    "Planned machine learning models:\n",
    "\n",
    "1. **Decision Tree** - Are relatively fast to train and can handle large datasets, which makes them suitable for this problem.\n",
    "\n",
    "2. **K-Nearest Neighbours (KNN)** - Can handle non-linear data and does not make assumptions about the distribution of the data.\n",
    "\n",
    "3. **Neural Networks** - Can be used for both classification and regression tasks, and can often achieve high levels of accuracy with appropriate training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the dependencies: Gymnasium, numpy, and random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import copy as copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Importing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section we are loading the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('sample.csv')\n",
    "print(f\"There are {len(sample)} samples.\")\n",
    "print(f\"This the following format:\")\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This the following format:\")\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few rows of the dataset:\")\n",
    "sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overview of the dataset:\")\n",
    "sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the output table, we can check that our dataset is composed by 20 attributes (columns) and it is visible that every column are numerical features ( all `int64` besides the pdfsize attribute's that are represented by `float64`). As we can see above, the dataset does not contain null values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Column Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **pdfsize** - The PDF size in Megabytes\n",
    "2. **pages** - Pages\n",
    "3. **title characters** - The number of characters in the title\n",
    "4. **Images** - The number of images\n",
    "5. **obj** - The number of keywords /obj\n",
    "6. **endobj** - The number of keywords /endobj\n",
    "7. **stream** - The number of keywords /stream\n",
    "8. **endstream** - The number of keywords /endstream\n",
    "9. **xref** - The number of xref tables\n",
    "10. **trailer** - The number of keywords /trailer\n",
    "11. **startxref** - The number of keywords /startxref\n",
    "12. **ObjStm** - The number of keywords /Objstm (Object Streams)\n",
    "13. **JS** - The number of keywords /JS\n",
    "14. **OBS_JS** - The number of keywords /JS (obfuscated)\n",
    "15. **Javascript** - The number of keywords /Javascript\n",
    "16. **OBS_Javascript** - The number of keywords /Javascript (obfuscated)\n",
    "17. **OpenAction** - The number of keywords /OpenAction\n",
    "18. **OBS_OpenAction** - The number of keywords /OpenAction (obfuscated)\n",
    "19. **Acroform** - The number of keywords /Acroform\n",
    "20. **OBS_Acroform** - The number of keywords /Acroform (obfuscated)\n",
    "21. **class** - Benign (0) or malicious (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Summary statistics of the dataset:\")\n",
    "sample.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **count** row shows the number of non-null entries in each column of the dataset.\n",
    "- The **mean** rows the average value of each column.\n",
    "- The **std** shows the standard deviation of each column, displaying how much the values deviate from the mean.\n",
    "- The **25%**, **50%** and **75%** rows show the first quartile, the median quartile and the third quartile values for each column.\n",
    "- The **min** and **max** rows show the minimum and maximum values in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Target Variable Distribution:\")\n",
    "sample['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The class = **1** represents the malicious files\n",
    "- The class = **0** represents the benign files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_sample = sample[sample['class'] == 1]\n",
    "benign_sample = sample[sample['class'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_count = len(malicious_sample)\n",
    "benign_count = len(benign_sample)\n",
    "\n",
    "data = [malicious_count, benign_count]  \n",
    "colors = (\"#eaac8b\", \"#6d597a\") \n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(data, colors=colors, labels=['Malicious', 'Benign'], autopct='%1.1f%%')\n",
    "ax1.axis('equal') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 20))\n",
    "sample.hist(figsize=(16, 20), color='#003f5c')\n",
    "plt.xlabel(\"Frequency\", fontsize=14)\n",
    "plt.ylabel(\"Number of Samples\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Malicious vs Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_sample = sample[sample['class'] == 1]\n",
    "benign_sample = sample[sample['class'] == 0]\n",
    "num_features = sample.shape[1] - 1 \n",
    "\n",
    "fig, axs = plt.subplots(num_features, 2, figsize=(16, num_features * 4))\n",
    "for i, column in enumerate(sample.columns):\n",
    "    if column == 'class':\n",
    "        continue\n",
    "    sb.histplot(malicious_sample[column], label='Malicious', color='#eaac8b', kde=True, ax=axs[i, 0], bins=30, alpha=0.6)\n",
    "    sb.histplot(benign_sample[column], label='Benign', color='#6d597a', kde=True, ax=axs[i, 0], bins=30, alpha=0.6)\n",
    "    axs[i, 0].set_title(f'{column} Distribution')\n",
    "    axs[i, 0].set_xlabel(column)\n",
    "    axs[i, 0].set_ylabel('Frequency')\n",
    "    axs[i, 0].legend()\n",
    "    sb.boxplot(x='class', y=column, data=sample, ax=axs[i, 1])\n",
    "    axs[i, 1].set_title(f'{column} Boxplot')\n",
    "    axs[i, 1].set_xlabel('Class')\n",
    "    axs[i, 1].set_ylabel(column)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = sample.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n",
    "\n",
    "# Drop rows with any missing values\n",
    "sample_cleaned_rows = sample.dropna(axis=0)\n",
    "\n",
    "# Display the shape of the DataFrame before and after dropping rows\n",
    "print(\"Shape before dropping rows:\", sample.shape)\n",
    "print(\"Shape after dropping rows:\", sample_cleaned_rows.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Removing Redundant Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can observe another interesting feature: the 'obj' column and the 'endobj' column, as well as the 'stream' column and the 'endstream' column, exhibit similar results. To prevent potential redundancy and ensure accurate analysis, we will also remove the 'endobj' and 'endstream' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant columns\n",
    "sample = sample.drop(columns=['endobj', 'endstream'])\n",
    "print(\"Shape of DataFrame after removing columns:\", sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Data Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"New summary statistics of the dataset:\")\n",
    "sample.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **count** row shows the number of non-null entries in each column of the dataset.\n",
    "- The **mean** rows the average value of each column.\n",
    "- The **std** shows the standard deviation of each column, displaying how much the values deviate from the mean.\n",
    "- The **25%**, **50%** and **75%** rows show the first quartile, the median quartile and the third quartile values for each column.\n",
    "- The **min** and **max** rows show the minimum and maximum values in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Pair Plot for Best Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will focus on visualizing the relationship between the selected best features using a pair plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the fraction of data to sample\n",
    "sample_fraction = 0.1\n",
    "\n",
    "# Randomly sample a fraction of the dataset\n",
    "sampled_data = sample.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "# Display the shape of the sampled dataset\n",
    "print(\"Shape of Sampled Data:\", sampled_data.shape)\n",
    "\n",
    "best_features = ['obj', 'pdfsize', 'pages', 'stream', 'Javascript', 'Acroform', 'class']\n",
    "\n",
    "# Pair plot for the sampled data\n",
    "pair_plottable_data = sampled_data[best_features]\n",
    "sb.pairplot(pair_plottable_data, hue='class')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this graph, we can easily visualize the relationships between the selected features (obj, pdfsize, pages, stream, Javascript, Acroform) and their distribution concerning the 'class' variable, aiding in exploratory data analysis and potentially providing insights into patterns or correlations within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of determining which features to remove to improve machine learning model performance involves a combination of exploratory data analysis, feature selection techniques, and domain knowledge.\n",
    "\n",
    "Highly correlated features can sometimes be redundant. Using correlation heatmaps can help identify these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = sample.corr()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sb.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from the Correlation Matrix:\n",
    "\n",
    "- **Empty Columns**: There are columns with no values, indicating zero correlation with all other columns in our dataset. These columns do not contribute to the model and should be removed.\n",
    "\n",
    "- **Low Correlation Values**: Some columns, especially those with a correlation value as low as 0.01 with our target variable (class), have minimal or no impact on the dataset. These columns can be excluded to reduce noise and improve model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Identifying and Removing Low-Variance Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand our dataset, we will examine the distribution of the columns that showed no correlation in the correlation matrix. These columns include OBS_JS, OBS_Javascript, OBS_OpenAction, and OBS_Acroform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot distribution of OBS_JS\n",
    "sb.histplot(sample['OBS_JS'], bins=30, kde=True, ax=axs[0, 0], color='skyblue')\n",
    "axs[0, 0].set_title('Distribution of OBS_JS')\n",
    "axs[0, 0].set_xlabel('OBS_JS')\n",
    "axs[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot distribution of OBS_Javascript\n",
    "sb.histplot(sample['OBS_Javascript'], bins=30, kde=True, ax=axs[0, 1], color='salmon')\n",
    "axs[0, 1].set_title('Distribution of OBS_Javascript')\n",
    "axs[0, 1].set_xlabel('OBS_Javascript')\n",
    "axs[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Plot distribution of OBS_OpenAction\n",
    "sb.histplot(sample['OBS_OpenAction'], bins=30, kde=True, ax=axs[1, 0], color='green')\n",
    "axs[1, 0].set_title('Distribution of OBS_OpenAction')\n",
    "axs[1, 0].set_xlabel('OBS_OpenAction')\n",
    "axs[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot distribution of OBS_Acroform\n",
    "sb.histplot(sample['OBS_Acroform'], bins=30, kde=True, ax=axs[1, 1], color='purple')\n",
    "axs[1, 1].set_title('Distribution of OBS_Acroform')\n",
    "axs[1, 1].set_xlabel('OBS_Acroform')\n",
    "axs[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plots, these four columns have constant values (all values are the same), meaning they don't provide any useful information for our analysis or modeling process.\n",
    "\n",
    "To improve the predictive power of our model and reduce noise, we will remove these columns from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the specified columns\n",
    "columns_to_remove = ['OBS_JS', 'OBS_Javascript', 'OBS_OpenAction', 'OBS_Acroform']\n",
    "sample = sample.drop(columns=columns_to_remove)\n",
    "\n",
    "# Check the shape of the DataFrame after removing the columns\n",
    "print(\"Shape of DataFrame after removing columns:\", sample.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a column that should be remove because of how low is it correlation value: ObjStm. As we can see the correlation matrix, the values between this column and the others are really low, specially with our main column (class), with a correlation value of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the specified columns\n",
    "columns_to_remove = ['ObjStm']\n",
    "sample = sample.drop(columns=columns_to_remove)\n",
    "\n",
    "# Check the shape of the DataFrame after removing the columns\n",
    "print(\"Shape of DataFrame after removing columns:\", sample.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Addressing Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can see that our dataset is unbalenced because there is a significant difference between the values of the binary feature ('class' column). \n",
    "\n",
    "There are 50000 0's (or no's) and 450000 1's (or yes's). \n",
    "\n",
    "To balance our dataset, we will use **Synthetic Minority Over-sampling Technique (SMOTE)** for oversampling and **RandomUnderSampler** for undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Techniques to Handle Imbalanced Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Values of the feature column ('class')\")\n",
    "sample['class'].value_counts()\n",
    "sample_undersample = copy.deepcopy(sample)\n",
    "sample_original = copy.deepcopy(sample)\n",
    "sample_best = copy.deepcopy(sample[['obj', 'pdfsize', 'pages', 'stream', 'Javascript', 'Acroform', 'class']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1 Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=42)\n",
    "X = sample.drop('class', axis=1)\n",
    "y = sample['class']\n",
    "X_res_undersampling, y_res_undersampling = rus.fit_resample(X, y)\n",
    "print(\"Values of the feature column ('class') after undersampling\")\n",
    "y_res_undersampling.value_counts()\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_best = sample_best.drop('class', axis=1)\n",
    "y_best = sample_best['class']\n",
    "X_res_best_undersampling, y_res_best_undersampling = rus.fit_resample(X_best, y_best)\n",
    "print(\"Values of the feature column ('class') after undersampling\")\n",
    "y_res_best_undersampling.value_counts()\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_under = sample_undersample.drop('class', axis=1)\n",
    "y_under = sample_undersample['class']\n",
    "X_res_sample_undersampling, y_res_sample_undersampling = rus.fit_resample(X_under, y_under)\n",
    "print(\"Values of the feature column ('class') after undersampling\")\n",
    "y_res_sample_undersampling.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the result output, we now have an undersampled balanced dataset because we have similar numbers of no's and yes's (0's and 1's, respectively). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2 Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_res_overslampling, y_res_oversampling = smote.fit_resample(X, y)\n",
    "X_res_best_overslampling, y_res_best_oversampling = smote.fit_resample(X_best, y_best)\n",
    "print(\"Values of the feature column ('class') after oversampling\")\n",
    "y_res_oversampling.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the result output, we now have an oversampled balanced dataset because we have similar numbers of no's and yes's (0's and 1's, respectively). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Splitting Data into Traning and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target variable (y)\n",
    "\n",
    "X_original = sample_original.drop('class', axis=1)\n",
    "y_original = sample_original['class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(X_under, y_under, test_size=0.2, random_state=100)\n",
    "\n",
    "X_train_original, X_test_original, y_train_original, y_test_original = train_test_split(X_original, y_original, test_size=0.2, random_state=83)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Training Multiple Supervised Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section where we'll train our models to predict the type of evasive of the pdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Columns are the columns we will use for input in the model\n",
    "input_cols = ['pdfsize', 'pages', 'title characters', 'images', 'obj', 'stream', 'xref', 'trailer', 'startxref', 'ObjStm', 'JS', 'Javascript', 'OpenAction', 'Acroform']\n",
    "\n",
    "# Input Columns are the options for the output of the model\n",
    "output_cols = ['Benign', 'Malicious']\n",
    "\n",
    "# Averages to calculate for Precision, Recall, and F1-score\n",
    "averages = ['binary', 'micro', 'macro', 'weighted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Model function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, x_test, y_test):\n",
    "    # Predict the target values for the test data\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate accuracy, precision, recall, f1-score and kappa score\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    conf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model_data, classifier, testSize):\n",
    "    # split data for trainging and testing\n",
    "    x = model_data.drop(columns = ['class'], axis = 1)\n",
    "    target = model_data['class']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, target, test_size = testSize, random_state = 42)\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "    # Evaluate the classifier on the test data\n",
    "    values = test_model(classifier, x_test, y_test)\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_algorithm(dataset, test_size=0.2):\n",
    "    # Model using Decision Tree algorithm\n",
    "    decision_tree_classifier = tree.DecisionTreeClassifier(random_state = 0)\n",
    "\n",
    "    (accuracy, precision, recall, f1, conf_matrix) = train_classifier(dataset, decision_tree_classifier, test_size)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, conf_matrix, decision_tree_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_values = decision_tree_algorithm(sample)\n",
    "(accuracy, precision, recall, f1, conf_matrix, decision_tree_classifier) = decision_tree_values\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_matrix, display_labels = decision_tree_classifier.classes_)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a breakdown of the results from the confusion matrix:\n",
    "\n",
    "- True Positive (TP): 90,000 cases were correctly predicted as malicious.\n",
    "- True Negative (TN): 9,964 cases were correctly predicted as benign.\n",
    "- False Positive (FP): 60 cases were incorrectly predicted as malicious when they were actually benign.\n",
    "- False Negative (FN): 74 cases were incorrectly predicted as benign when they were actually malicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_values = decision_tree_algorithm(sample_best)\n",
    "(accuracy, precision, recall, f1, conf_matrix, decision_tree_classifier) = decision_tree_values\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_matrix, display_labels = decision_tree_classifier.classes_)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations:\n",
    "\n",
    "**High Accuracy:** With an accuracy of 0.99866, the model correctly predicts most of the samples, both benign and malicious.\n",
    "\n",
    "**High Precision and Recall:** Precision of 0.99933 for class 1 indicates that almost all PDFs predicted as malicious are indeed malicious, and a recall of 0.99918 means the model is excellent at detecting almost all malicious PDFs in the dataset.\n",
    "\n",
    "**F1 Score:** The F1 score, which balances precision and recall, is also very high at 0.99925, indicating that the model's overall accuracy in terms of precision and recall balance is outstanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.2. K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_algorithm(dataset, test_size=0.2, n_neighbors=5):\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    accuracy, precision, recall, f1, conf_matrix = train_classifier(dataset, knn_classifier, test_size)\n",
    "   \n",
    "    return accuracy, precision, recall, f1, conf_matrix, knn_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_values = knn_algorithm(sample, test_size=0.2, n_neighbors=5)\n",
    "accuracy, precision, recall, f1, conf_matrix, knn_classifier = knn_values\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=knn_classifier.classes_)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_values_best = knn_algorithm(sample_best, test_size=0.2, n_neighbors=5)\n",
    "accuracy_best, precision_best, recall_best, f1_best, conf_matrix_best, knn_classifier_best = knn_values_best\n",
    "\n",
    "print(\"Accuracy:\", accuracy_best)\n",
    "print(\"Precision:\", precision_best)\n",
    "print(\"Recall:\", recall_best)\n",
    "print(\"F1 score:\", f1_best)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=conf_matrix_best, display_labels=knn_classifier.classes_)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.3 Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_networks_algorithm(dataset, test_size=0.2):\n",
    "    # Create the classifier object\n",
    "    neural_networks_classifier = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=200, alpha=0.0001, solver='adam', verbose=0, random_state=0, tol=0.000000001)\n",
    "\n",
    "    (accuracy, precision, recall, f1, conf_matrix) = train_classifier(dataset, neural_networks_classifier, test_size)\n",
    "\n",
    "    return accuracy, precision, recall, f1, conf_matrix, neural_networks_classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_networks_data_best = neural_networks_algorithm(sample_best)\n",
    "(accuracy_best, precision_best, recall_best, f1_best, conf_matrix_best, neural_networks_classifier_best) = neural_networks_data_best\n",
    "\n",
    "\n",
    "neural_networks_data = neural_networks_algorithm(sample)\n",
    "(accuracy, precision, recall, f1, conf_matrix, neural_networks_classifier) = neural_networks_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural networks confusion matrix:\n",
    "\n",
    "#sample_best:\n",
    "\n",
    "print(\"Accuracy:\", accuracy_best)\n",
    "print(\"Precision:\", precision_best)\n",
    "print(\"Recall:\", recall_best)\n",
    "print(\"F1 score:\", f1_best)\n",
    "\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_matrix_best, display_labels = decision_tree_classifier.classes_)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample:\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n",
    "\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_matrix, display_labels = decision_tree_classifier.classes_)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations:\n",
    "\n",
    "**Accuracy:** Sample with 0.99871 and Sample_best with 0.99614. The normal sample has higher accuracy (0.99871 vs. 0.99614).\n",
    "\n",
    "**Precision:** Sample with 0.9990668221963006 and Sample_best with 0.998653041232523. The normal sample has higher precision (0.99907 vs. 0.99865), indicating it better avoids false positives.\n",
    "\n",
    "**Recall:** Sample with 0.9994998666311016 and Sample_best with 0.9970547701609318. The normal sample has higher recall (0.99950 vs. 0.99705), meaning it is better at identifying true positives.\n",
    "\n",
    "**F1 Score:** Sample with 0.9992832974982082 and Sample_best with 0.9978532657056416. The normal sample has a higher F1 score (0.99928 vs. 0.99785), indicating better overall performance in balancing precision and recall.\n",
    "\n",
    "From the sample and sample_best datasets we can conclude that there was a loss of information since the the sample has better performance in all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.2 Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the original sample:\n",
    "\n",
    "As we can see in the images, the model with best precision is ?? and ... is the ... .\n",
    "\n",
    "For the sample with the 6 best features:\n",
    "\n",
    "As we can see in the images, the model with best precision is ?? and ... is the ... ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.3 Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the original sample:\n",
    "\n",
    "As we can see in the images, the model with best recall is ?? and ... is the ... .\n",
    "\n",
    "For the sample with the 6 best features:\n",
    "\n",
    "As we can see in the images, the model with best recall is ?? and ... is the ... ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.4 Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the original sample:\n",
    "\n",
    "As we can see in the images, the model with best accuracy is ?? and ... is the ... .\n",
    "\n",
    "For the sample with the 6 best features:\n",
    "\n",
    "As we can see in the images, the model with best accuracy is ?? and ... is the ... .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.5 F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the original sample:\n",
    "\n",
    "As we can see in the images, the model with best f1 score is ?? and ... is the ... .\n",
    "\n",
    "For the sample with the 6 best features:\n",
    "\n",
    "As we can see in the images, the model with best f1 score is ?? and ... is the ... ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Comparing Model Performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
